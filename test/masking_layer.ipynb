{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda, Dense, Masking\n",
    "from keras.layers import Input, Embedding, LSTM\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "#from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setsum_layer\n",
    "reload(setsum_layer)\n",
    "from setsum_layer import SetSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_timestep = 4\n",
    "n_feature = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_examples = 10000\n",
    "max_train_length = 10\n",
    "\n",
    "num_test_examples = 10000\n",
    "min_test_length=5\n",
    "max_test_length=100\n",
    "step_test_length=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros((num_train_examples,max_train_length))\n",
    "sum_X = np.zeros((num_train_examples))\n",
    "for i in range(num_train_examples):\n",
    "    n = np.random.randint(1,max_train_length)\n",
    "    for j in range(1,n+1):\n",
    "        X[i,-j] = np.random.randint(1,10)\n",
    "    sum_X[i] = np.sum(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(max_train_length, ))\n",
    "x = Embedding(10, n_feature, mask_zero=True)(inputs)\n",
    "x = SetSum(5)(x)\n",
    "out = Dense(1)(x)\n",
    "model = Model(inputs=inputs, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x25c13c5a908>,\n",
       " <keras.layers.embeddings.Embedding at 0x25c13c5a9b0>,\n",
       " <setsum_layer.SetSum at 0x25c13c5aa20>,\n",
       " <keras.layers.core.Dense at 0x25c13c5ac18>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8765 samples, validate on 1235 samples\n",
      "Epoch 1/5\n",
      "8765/8765 [==============================] - 0s 11us/step - loss: 2.0130 - val_loss: 1.4150\n",
      "Epoch 2/5\n",
      "8765/8765 [==============================] - 0s 11us/step - loss: 0.8900 - val_loss: 0.2842\n",
      "Epoch 3/5\n",
      "8765/8765 [==============================] - 0s 11us/step - loss: 0.1105 - val_loss: 0.0760\n",
      "Epoch 4/5\n",
      "8765/8765 [==============================] - 0s 11us/step - loss: 0.0689 - val_loss: 0.0604\n",
      "Epoch 5/5\n",
      "8765/8765 [==============================] - 0s 11us/step - loss: 0.0515 - val_loss: 0.0426\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X, sum_X, epochs=5, batch_size=128, shuffle=True, validation_split=0.123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_11/NotEqual:0' shape=(?, 10) dtype=bool>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_mat = model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.037888  ,  0.02562937, -0.03066218],\n",
       "       [-0.03654822, -0.01050846,  0.0360505 ],\n",
       "       [-0.00666766,  0.02890015, -0.0112601 ],\n",
       "       [ 0.01048945,  0.02720269, -0.03302656],\n",
       "       [ 0.00501064, -0.02895998,  0.00603358],\n",
       "       [ 0.02637167,  0.01984045, -0.02812632],\n",
       "       [-0.00775652, -0.01738523,  0.00421908],\n",
       "       [-0.03398637, -0.00293621, -0.04882621],\n",
       "       [ 0.03465379, -0.01851556,  0.00933194],\n",
       "       [ 0.04919601,  0.00705652, -0.0106264 ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.037888  ,  0.02562937, -0.03066218],\n",
       "       [-0.03613896, -0.00655953,  0.03242232],\n",
       "       [ 0.05397358, -0.03181753, -0.07440148],\n",
       "       [ 0.1365238 , -0.10759641, -0.16054569],\n",
       "       [ 0.21852352, -0.2678932 , -0.2072292 ],\n",
       "       [ 0.30838066, -0.30277041, -0.3082521 ],\n",
       "       [ 0.37281662, -0.461725  , -0.37217689],\n",
       "       [ 0.41763115, -0.53207082, -0.4946945 ],\n",
       "       [ 0.56430769, -0.63360906, -0.51339316],\n",
       "       [ 0.65263557, -0.68722349, -0.60617489]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  5.,  6.,  3.,  9.],\n",
       "       [ 0.,  4.,  6.,  1.,  8.,  5.,  6.,  3.,  7.,  8.],\n",
       "       [ 0.,  3.,  5.,  4.,  9.,  3.,  3.,  4.,  2.,  9.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  3.,  2.,  6.,  4.,  3.]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10518206,  0.27263886,  0.06472662], dtype=float32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_mat[2, :]+embed_mat[6, :]+embed_mat[3, :]*2+embed_mat[4, :]+embed_mat[0, :]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07625498, -0.00588461,  0.08142723],\n",
       "       [ 0.08509673, -0.08664991,  0.18796268],\n",
       "       [ 0.16553947,  0.11674161,  0.02120311],\n",
       "       [ 0.06303501,  0.05118224,  0.01142141]], dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[:4, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deepset_model(max_length):\n",
    "    input_txt = Input(shape=(max_length,))\n",
    "    x = Embedding(10, 10, mask_zero=True)(input_txt)\n",
    "    #x = Dense(30, activation='tanh')(x)\n",
    "    Adder = Lambda(lambda x: K.sum(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])))\n",
    "    x = Adder(x)\n",
    "    encoded = Dense(1)(x)\n",
    "    summer = Model(input_txt, encoded)\n",
    "    adam = Adam(lr=1e-4, epsilon=1e-3)\n",
    "    summer.compile(optimizer=adam, loss='mae')\n",
    "    return summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lstm_model(max_length):\n",
    "    input_txt = Input(shape=(max_length,))\n",
    "    x = Embedding(10, 10, mask_zero=True)(input_txt)\n",
    "    x = LSTM(50)(x)\n",
    "    encoded = Dense(1)(x)\n",
    "    summer = Model(input_txt, encoded)\n",
    "    adam = Adam(lr=1e-4)\n",
    "    summer.compile(optimizer=adam, loss='mae')\n",
    "    return summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_deepset_model(max_train_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_lstm_model(max_train_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_layer = model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_10/NotEqual:0' shape=(?, 10) dtype=bool>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_layer.output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08035915,  0.10173114,  0.07942988,  0.06977315,  0.08350004,\n",
       "        0.09210253,  0.09292872,  0.06657916,  0.07745726,  0.10820688], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(model.layers[1].get_weights()[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9876 samples, validate on 124 samples\n",
      "Epoch 1/10\n",
      "9876/9876 [==============================] - 1s 75us/step - loss: 25.4291 - val_loss: 22.3197\n",
      "Epoch 2/10\n",
      "9876/9876 [==============================] - 0s 16us/step - loss: 25.1853 - val_loss: 22.0711\n",
      "Epoch 3/10\n",
      "9876/9876 [==============================] - 0s 19us/step - loss: 24.9310 - val_loss: 21.8090\n",
      "Epoch 4/10\n",
      "9876/9876 [==============================] - 0s 24us/step - loss: 24.6602 - val_loss: 21.5283\n",
      "Epoch 5/10\n",
      "9876/9876 [==============================] - 0s 16us/step - loss: 24.3687 - val_loss: 21.2251\n",
      "Epoch 6/10\n",
      "9876/9876 [==============================] - 0s 15us/step - loss: 24.0584 - val_loss: 20.9017\n",
      "Epoch 7/10\n",
      "9876/9876 [==============================] - 0s 17us/step - loss: 23.7278 - val_loss: 20.5507\n",
      "Epoch 8/10\n",
      "9876/9876 [==============================] - 0s 19us/step - loss: 23.3693 - val_loss: 20.1711\n",
      "Epoch 9/10\n",
      "9876/9876 [==============================] - 0s 20us/step - loss: 22.9906 - val_loss: 19.7771\n",
      "Epoch 10/10\n",
      "9876/9876 [==============================] - 0s 25us/step - loss: 22.5960 - val_loss: 19.3580\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X, sum_X, epochs=10, batch_size=128, shuffle=True, validation_split=0.0123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25735557,  0.33120143,  0.26050481,  0.24138205,  0.28021345,\n",
       "        0.27690503,  0.22618987,  0.24287578,  0.26898557,  0.2789948 ], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(model.layers[1].get_weights()[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SumMaskedEmbed(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(SumMaskedEmbed, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, 'float32')\n",
    "            if not len(mask.shape) == 2:\n",
    "                print(\"Mask shape:\", mask.shape)\n",
    "            mask = K.repeat(mask, x.shape[-1])\n",
    "            mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "            masked = mask*x\n",
    "            return K.sum(masked, axis=1)\n",
    "        else:\n",
    "            return K.sum(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SetSum(Layer):\n",
    "    \"\"\"The DeepSet operation: apply a dense layer on the last axis, and then do a masked sum on axis=1. \n",
    "\n",
    "    # Arguments\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use\n",
    "            (see [activations](../activations.md)).\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix\n",
    "            (see [initializers](../initializers.md)).\n",
    "        bias_initializer: Initializer for the bias vector\n",
    "            (see [initializers](../initializers.md)).\n",
    "        kernel_regularizer: Regularizer function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        bias_regularizer: Regularizer function applied to the bias vector\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        activity_regularizer: Regularizer function applied to\n",
    "            the output of the layer (its \"activation\").\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        kernel_constraint: Constraint function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        bias_constraint: Constraint function applied to the bias vector\n",
    "            (see [constraints](../constraints.md)).\n",
    "    # Input shape\n",
    "        The input should have the dimensions: (batch, timestep, feature). The last layer should pass a mask with \n",
    "        dimension (batch, timestep), which is the default output_mask from Embedding(mask_zero=True)\n",
    "    # Output shape\n",
    "        (batch, feature). Note that the timestep dimension is summed out. \n",
    "    \"\"\"\n",
    "    def __init__(self, units,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(SetSum, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        output = K.dot(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "            \n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, 'float32')\n",
    "            if not len(mask.shape) == 2:\n",
    "                print(\"Mask shape:\", mask.shape)\n",
    "            mask = K.repeat(mask, units)\n",
    "            mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "            output = output*mask\n",
    "            \n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 3\n",
    "        assert input_shape[-1]\n",
    "        return (input_shape[0], self.units)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(Dense, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
